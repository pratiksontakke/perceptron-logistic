---
description: 
globs: 
alwaysApply: true
---
# Assignment: Perceptron From Scratch

## ðŸ§  Task Overview

You're required to implement a **logistic regression model** (single-neuron perceptron) using **pure NumPy**. You'll train the model to classify fruits using features from `fruit.csv`.

---

## ðŸ“‚ Folder Structure

```
q3/
â”œâ”€â”€ perceptron.ipynb       # Jupyter notebook with all implementation & plots
â”œâ”€â”€ fruit.csv              # Input dataset (â‰¥12 rows)
â””â”€â”€ reflection.md          # Write-up answering prompts (â‰¤ 300 words)
```

---

## ðŸ“Š Dataset: fruit.csv

- Minimum 12 rows
- Columns:
  - `length_cm`: float
  - `weight_g`: float
  - `yellow_score`: float (0â€“1)
  - `label`: int (`0` for apple, `1` for banana)

---

## ðŸ’» Implementation Requirements

- Use **only NumPy**
- Logistic Regression (binary classification)
- Batch gradient descent
  - Train for â‰¥ 500 epochs **or** until **loss < 0.05**
- Plot the following:
  - Loss vs. Epoch
  - Accuracy vs. Epoch

---

## âœï¸ Reflection.md (â‰¤ 300 words)

Answer the following:
- How did initial random predictions differ from the final trained model?
- How did learning rate affect convergence?
- Link to "DJ knob / child-learning" analogy (intuitive explanation of learning rate adjustment).




